{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT210x - Programming with Python for DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module5- Lab5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "matplotlib.style.use('ggplot') # Look Pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Convenience Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDecisionBoundary(model, X, y):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    padding = 0.6\n",
    "    resolution = 0.0025\n",
    "    colors = ['royalblue','forestgreen','ghostwhite']\n",
    "\n",
    "    # Calculate the boundaris\n",
    "    x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "    y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "    x_range = x_max - x_min\n",
    "    y_range = y_max - y_min\n",
    "    x_min -= x_range * padding\n",
    "    y_min -= y_range * padding\n",
    "    x_max += x_range * padding\n",
    "    y_max += y_range * padding\n",
    "\n",
    "    # Create a 2D Grid Matrix. The values stored in the matrix\n",
    "    # are the predictions of the class at at said location\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, resolution),\n",
    "                       np.arange(y_min, y_max, resolution))\n",
    "\n",
    "    # What class does the classifier say?\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot the contour map\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.terrain)\n",
    "\n",
    "    # Plot the test original points as well...\n",
    "    for label in range(len(np.unique(y))):\n",
    "        indices = np.where(y == label)\n",
    "        plt.scatter(X[indices, 0], X[indices, 1], c=colors[label], label=str(label), alpha=0.8)\n",
    "\n",
    "    p = model.get_params()\n",
    "    plt.axis('tight')\n",
    "    plt.title('K = ' + str(p['n_neighbors']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Load up the dataset into a variable called `X`. Check `.head` and `dtypes` to make sure you're loading your data properly--don't fail on the 1st step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               int64\n",
       "area           float64\n",
       "perimeter      float64\n",
       "compactness    float64\n",
       "length         float64\n",
       "width          float64\n",
       "asymmetry      float64\n",
       "groove         float64\n",
       "wheat_type      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .. your code here ..\n",
    "X = pd.read_csv(\"Datasets/wheat.data\")\n",
    "X.head()\n",
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the `wheat_type` series slice out of `X`, and into a series called `y`. Then drop the original `wheat_type` column from the `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()\n",
    "y = X[\"wheat_type\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a quick, \"ordinal\" conversion of `y`. In actuality our classification isn't ordinal, but just as an experiment..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>compactness</th>\n",
       "      <th>length</th>\n",
       "      <th>width</th>\n",
       "      <th>asymmetry</th>\n",
       "      <th>groove</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.26</td>\n",
       "      <td>14.84</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>5.763</td>\n",
       "      <td>3.312</td>\n",
       "      <td>2.2210</td>\n",
       "      <td>5.220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.88</td>\n",
       "      <td>14.57</td>\n",
       "      <td>0.8811</td>\n",
       "      <td>5.554</td>\n",
       "      <td>3.333</td>\n",
       "      <td>1.0180</td>\n",
       "      <td>4.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.29</td>\n",
       "      <td>14.09</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>5.291</td>\n",
       "      <td>3.337</td>\n",
       "      <td>2.6990</td>\n",
       "      <td>4.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.84</td>\n",
       "      <td>13.94</td>\n",
       "      <td>0.8955</td>\n",
       "      <td>5.324</td>\n",
       "      <td>3.379</td>\n",
       "      <td>2.2590</td>\n",
       "      <td>4.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.14</td>\n",
       "      <td>14.99</td>\n",
       "      <td>0.9034</td>\n",
       "      <td>5.658</td>\n",
       "      <td>3.562</td>\n",
       "      <td>1.3550</td>\n",
       "      <td>5.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.38</td>\n",
       "      <td>14.21</td>\n",
       "      <td>0.8951</td>\n",
       "      <td>5.386</td>\n",
       "      <td>3.312</td>\n",
       "      <td>2.4620</td>\n",
       "      <td>4.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.69</td>\n",
       "      <td>14.49</td>\n",
       "      <td>0.8799</td>\n",
       "      <td>5.563</td>\n",
       "      <td>3.259</td>\n",
       "      <td>3.5860</td>\n",
       "      <td>5.219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.11</td>\n",
       "      <td>14.10</td>\n",
       "      <td>0.8911</td>\n",
       "      <td>5.420</td>\n",
       "      <td>3.302</td>\n",
       "      <td>2.7000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16.63</td>\n",
       "      <td>15.46</td>\n",
       "      <td>0.8747</td>\n",
       "      <td>6.053</td>\n",
       "      <td>3.465</td>\n",
       "      <td>2.0400</td>\n",
       "      <td>5.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16.44</td>\n",
       "      <td>15.25</td>\n",
       "      <td>0.8880</td>\n",
       "      <td>5.884</td>\n",
       "      <td>3.505</td>\n",
       "      <td>1.9690</td>\n",
       "      <td>5.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15.26</td>\n",
       "      <td>14.85</td>\n",
       "      <td>0.8696</td>\n",
       "      <td>5.714</td>\n",
       "      <td>3.242</td>\n",
       "      <td>4.5430</td>\n",
       "      <td>5.314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14.03</td>\n",
       "      <td>14.16</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>5.438</td>\n",
       "      <td>3.201</td>\n",
       "      <td>1.7170</td>\n",
       "      <td>5.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.89</td>\n",
       "      <td>14.02</td>\n",
       "      <td>0.8880</td>\n",
       "      <td>5.439</td>\n",
       "      <td>3.199</td>\n",
       "      <td>3.9860</td>\n",
       "      <td>4.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.78</td>\n",
       "      <td>14.06</td>\n",
       "      <td>0.8759</td>\n",
       "      <td>5.479</td>\n",
       "      <td>3.156</td>\n",
       "      <td>3.1360</td>\n",
       "      <td>4.872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13.74</td>\n",
       "      <td>14.05</td>\n",
       "      <td>0.8744</td>\n",
       "      <td>5.482</td>\n",
       "      <td>3.114</td>\n",
       "      <td>2.9320</td>\n",
       "      <td>4.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.59</td>\n",
       "      <td>14.28</td>\n",
       "      <td>0.8993</td>\n",
       "      <td>5.351</td>\n",
       "      <td>3.333</td>\n",
       "      <td>4.1850</td>\n",
       "      <td>4.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>13.99</td>\n",
       "      <td>13.83</td>\n",
       "      <td>0.9183</td>\n",
       "      <td>5.119</td>\n",
       "      <td>3.383</td>\n",
       "      <td>5.2340</td>\n",
       "      <td>4.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>15.69</td>\n",
       "      <td>14.75</td>\n",
       "      <td>0.9058</td>\n",
       "      <td>5.527</td>\n",
       "      <td>3.514</td>\n",
       "      <td>1.5990</td>\n",
       "      <td>5.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.70</td>\n",
       "      <td>14.21</td>\n",
       "      <td>0.9153</td>\n",
       "      <td>5.205</td>\n",
       "      <td>3.466</td>\n",
       "      <td>1.7670</td>\n",
       "      <td>4.649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>12.72</td>\n",
       "      <td>13.57</td>\n",
       "      <td>0.8686</td>\n",
       "      <td>5.226</td>\n",
       "      <td>3.049</td>\n",
       "      <td>4.1020</td>\n",
       "      <td>4.914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.16</td>\n",
       "      <td>14.40</td>\n",
       "      <td>0.8584</td>\n",
       "      <td>5.658</td>\n",
       "      <td>3.129</td>\n",
       "      <td>3.0720</td>\n",
       "      <td>5.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>14.11</td>\n",
       "      <td>14.26</td>\n",
       "      <td>0.8722</td>\n",
       "      <td>5.520</td>\n",
       "      <td>3.168</td>\n",
       "      <td>2.6880</td>\n",
       "      <td>5.219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.88</td>\n",
       "      <td>14.90</td>\n",
       "      <td>0.8988</td>\n",
       "      <td>5.618</td>\n",
       "      <td>3.507</td>\n",
       "      <td>0.7651</td>\n",
       "      <td>5.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>12.08</td>\n",
       "      <td>13.23</td>\n",
       "      <td>0.8664</td>\n",
       "      <td>5.099</td>\n",
       "      <td>2.936</td>\n",
       "      <td>1.4150</td>\n",
       "      <td>4.961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15.01</td>\n",
       "      <td>14.76</td>\n",
       "      <td>0.8657</td>\n",
       "      <td>5.789</td>\n",
       "      <td>3.245</td>\n",
       "      <td>1.7910</td>\n",
       "      <td>5.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16.19</td>\n",
       "      <td>15.16</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>5.833</td>\n",
       "      <td>3.421</td>\n",
       "      <td>0.9030</td>\n",
       "      <td>5.307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>13.02</td>\n",
       "      <td>13.76</td>\n",
       "      <td>0.8641</td>\n",
       "      <td>5.395</td>\n",
       "      <td>3.026</td>\n",
       "      <td>3.3730</td>\n",
       "      <td>4.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>12.74</td>\n",
       "      <td>13.67</td>\n",
       "      <td>0.8564</td>\n",
       "      <td>5.395</td>\n",
       "      <td>2.956</td>\n",
       "      <td>2.5040</td>\n",
       "      <td>4.869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>14.11</td>\n",
       "      <td>14.18</td>\n",
       "      <td>0.8820</td>\n",
       "      <td>5.541</td>\n",
       "      <td>3.221</td>\n",
       "      <td>2.7540</td>\n",
       "      <td>5.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>13.45</td>\n",
       "      <td>14.02</td>\n",
       "      <td>0.8604</td>\n",
       "      <td>5.516</td>\n",
       "      <td>3.065</td>\n",
       "      <td>3.5310</td>\n",
       "      <td>5.097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>11.41</td>\n",
       "      <td>12.95</td>\n",
       "      <td>0.8560</td>\n",
       "      <td>5.090</td>\n",
       "      <td>2.775</td>\n",
       "      <td>4.9570</td>\n",
       "      <td>4.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>12.46</td>\n",
       "      <td>13.41</td>\n",
       "      <td>0.8706</td>\n",
       "      <td>5.236</td>\n",
       "      <td>3.017</td>\n",
       "      <td>4.9870</td>\n",
       "      <td>5.147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>12.19</td>\n",
       "      <td>13.36</td>\n",
       "      <td>0.8579</td>\n",
       "      <td>5.240</td>\n",
       "      <td>2.909</td>\n",
       "      <td>4.8570</td>\n",
       "      <td>5.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>11.65</td>\n",
       "      <td>13.07</td>\n",
       "      <td>0.8575</td>\n",
       "      <td>5.108</td>\n",
       "      <td>2.850</td>\n",
       "      <td>5.2090</td>\n",
       "      <td>5.135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>12.89</td>\n",
       "      <td>13.77</td>\n",
       "      <td>0.8541</td>\n",
       "      <td>5.495</td>\n",
       "      <td>3.026</td>\n",
       "      <td>6.1850</td>\n",
       "      <td>5.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>11.56</td>\n",
       "      <td>13.31</td>\n",
       "      <td>0.8198</td>\n",
       "      <td>5.363</td>\n",
       "      <td>2.683</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>5.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>11.81</td>\n",
       "      <td>13.45</td>\n",
       "      <td>0.8198</td>\n",
       "      <td>5.413</td>\n",
       "      <td>2.716</td>\n",
       "      <td>4.8980</td>\n",
       "      <td>5.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>10.91</td>\n",
       "      <td>12.80</td>\n",
       "      <td>0.8372</td>\n",
       "      <td>5.088</td>\n",
       "      <td>2.675</td>\n",
       "      <td>4.1790</td>\n",
       "      <td>4.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>11.23</td>\n",
       "      <td>12.82</td>\n",
       "      <td>0.8594</td>\n",
       "      <td>5.089</td>\n",
       "      <td>2.821</td>\n",
       "      <td>7.5240</td>\n",
       "      <td>4.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>10.59</td>\n",
       "      <td>12.41</td>\n",
       "      <td>0.8648</td>\n",
       "      <td>4.899</td>\n",
       "      <td>2.787</td>\n",
       "      <td>4.9750</td>\n",
       "      <td>4.794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>10.93</td>\n",
       "      <td>12.80</td>\n",
       "      <td>0.8390</td>\n",
       "      <td>5.046</td>\n",
       "      <td>2.717</td>\n",
       "      <td>5.3980</td>\n",
       "      <td>5.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>11.27</td>\n",
       "      <td>12.86</td>\n",
       "      <td>0.8563</td>\n",
       "      <td>5.091</td>\n",
       "      <td>2.804</td>\n",
       "      <td>3.9850</td>\n",
       "      <td>5.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>11.87</td>\n",
       "      <td>13.02</td>\n",
       "      <td>0.8795</td>\n",
       "      <td>5.132</td>\n",
       "      <td>2.953</td>\n",
       "      <td>3.5970</td>\n",
       "      <td>5.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>10.82</td>\n",
       "      <td>12.83</td>\n",
       "      <td>0.8256</td>\n",
       "      <td>5.180</td>\n",
       "      <td>2.630</td>\n",
       "      <td>4.8530</td>\n",
       "      <td>5.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>12.11</td>\n",
       "      <td>13.27</td>\n",
       "      <td>0.8639</td>\n",
       "      <td>5.236</td>\n",
       "      <td>2.975</td>\n",
       "      <td>4.1320</td>\n",
       "      <td>5.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>12.80</td>\n",
       "      <td>13.47</td>\n",
       "      <td>0.8860</td>\n",
       "      <td>5.160</td>\n",
       "      <td>3.126</td>\n",
       "      <td>4.8730</td>\n",
       "      <td>4.914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>12.79</td>\n",
       "      <td>13.53</td>\n",
       "      <td>0.8786</td>\n",
       "      <td>5.224</td>\n",
       "      <td>3.054</td>\n",
       "      <td>5.4830</td>\n",
       "      <td>4.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>13.37</td>\n",
       "      <td>13.78</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>5.320</td>\n",
       "      <td>3.128</td>\n",
       "      <td>4.6700</td>\n",
       "      <td>5.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>12.62</td>\n",
       "      <td>13.67</td>\n",
       "      <td>0.8481</td>\n",
       "      <td>5.410</td>\n",
       "      <td>2.911</td>\n",
       "      <td>3.3060</td>\n",
       "      <td>5.231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>12.76</td>\n",
       "      <td>13.38</td>\n",
       "      <td>0.8964</td>\n",
       "      <td>5.073</td>\n",
       "      <td>3.155</td>\n",
       "      <td>2.8280</td>\n",
       "      <td>4.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>12.38</td>\n",
       "      <td>13.44</td>\n",
       "      <td>0.8609</td>\n",
       "      <td>5.219</td>\n",
       "      <td>2.989</td>\n",
       "      <td>5.4720</td>\n",
       "      <td>5.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>12.67</td>\n",
       "      <td>13.32</td>\n",
       "      <td>0.8977</td>\n",
       "      <td>4.984</td>\n",
       "      <td>3.135</td>\n",
       "      <td>2.3000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>11.18</td>\n",
       "      <td>12.72</td>\n",
       "      <td>0.8680</td>\n",
       "      <td>5.009</td>\n",
       "      <td>2.810</td>\n",
       "      <td>4.0510</td>\n",
       "      <td>4.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>12.70</td>\n",
       "      <td>13.41</td>\n",
       "      <td>0.8874</td>\n",
       "      <td>5.183</td>\n",
       "      <td>3.091</td>\n",
       "      <td>8.4560</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>12.37</td>\n",
       "      <td>13.47</td>\n",
       "      <td>0.8567</td>\n",
       "      <td>5.204</td>\n",
       "      <td>2.960</td>\n",
       "      <td>3.9190</td>\n",
       "      <td>5.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>12.19</td>\n",
       "      <td>13.20</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>5.137</td>\n",
       "      <td>2.981</td>\n",
       "      <td>3.6310</td>\n",
       "      <td>4.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>11.23</td>\n",
       "      <td>12.88</td>\n",
       "      <td>0.8511</td>\n",
       "      <td>5.140</td>\n",
       "      <td>2.795</td>\n",
       "      <td>4.3250</td>\n",
       "      <td>5.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>13.20</td>\n",
       "      <td>13.66</td>\n",
       "      <td>0.8883</td>\n",
       "      <td>5.236</td>\n",
       "      <td>3.232</td>\n",
       "      <td>8.3150</td>\n",
       "      <td>5.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>11.84</td>\n",
       "      <td>13.21</td>\n",
       "      <td>0.8521</td>\n",
       "      <td>5.175</td>\n",
       "      <td>2.836</td>\n",
       "      <td>3.5980</td>\n",
       "      <td>5.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>12.30</td>\n",
       "      <td>13.34</td>\n",
       "      <td>0.8684</td>\n",
       "      <td>5.243</td>\n",
       "      <td>2.974</td>\n",
       "      <td>5.6370</td>\n",
       "      <td>5.063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      area  perimeter  compactness  length  width  asymmetry  groove\n",
       "0    15.26      14.84       0.8710   5.763  3.312     2.2210   5.220\n",
       "1    14.88      14.57       0.8811   5.554  3.333     1.0180   4.956\n",
       "2    14.29      14.09       0.9050   5.291  3.337     2.6990   4.825\n",
       "3    13.84      13.94       0.8955   5.324  3.379     2.2590   4.805\n",
       "4    16.14      14.99       0.9034   5.658  3.562     1.3550   5.175\n",
       "5    14.38      14.21       0.8951   5.386  3.312     2.4620   4.956\n",
       "6    14.69      14.49       0.8799   5.563  3.259     3.5860   5.219\n",
       "7    14.11      14.10       0.8911   5.420  3.302     2.7000     NaN\n",
       "8    16.63      15.46       0.8747   6.053  3.465     2.0400   5.877\n",
       "9    16.44      15.25       0.8880   5.884  3.505     1.9690   5.533\n",
       "10   15.26      14.85       0.8696   5.714  3.242     4.5430   5.314\n",
       "11   14.03      14.16       0.8796   5.438  3.201     1.7170   5.001\n",
       "12   13.89      14.02       0.8880   5.439  3.199     3.9860   4.738\n",
       "13   13.78      14.06       0.8759   5.479  3.156     3.1360   4.872\n",
       "14   13.74      14.05       0.8744   5.482  3.114     2.9320   4.825\n",
       "15   14.59      14.28       0.8993   5.351  3.333     4.1850   4.781\n",
       "16   13.99      13.83       0.9183   5.119  3.383     5.2340   4.781\n",
       "17   15.69      14.75       0.9058   5.527  3.514     1.5990   5.046\n",
       "18   14.70      14.21       0.9153   5.205  3.466     1.7670   4.649\n",
       "19   12.72      13.57       0.8686   5.226  3.049     4.1020   4.914\n",
       "20   14.16      14.40       0.8584   5.658  3.129     3.0720   5.176\n",
       "21   14.11      14.26       0.8722   5.520  3.168     2.6880   5.219\n",
       "22   15.88      14.90       0.8988   5.618  3.507     0.7651   5.091\n",
       "23   12.08      13.23       0.8664   5.099  2.936     1.4150   4.961\n",
       "24   15.01      14.76       0.8657   5.789  3.245     1.7910   5.001\n",
       "25   16.19      15.16       0.8849   5.833  3.421     0.9030   5.307\n",
       "26   13.02      13.76       0.8641   5.395  3.026     3.3730   4.825\n",
       "27   12.74      13.67       0.8564   5.395  2.956     2.5040   4.869\n",
       "28   14.11      14.18       0.8820   5.541  3.221     2.7540   5.038\n",
       "29   13.45      14.02       0.8604   5.516  3.065     3.5310   5.097\n",
       "..     ...        ...          ...     ...    ...        ...     ...\n",
       "180  11.41      12.95       0.8560   5.090  2.775     4.9570   4.825\n",
       "181  12.46      13.41       0.8706   5.236  3.017     4.9870   5.147\n",
       "182  12.19      13.36       0.8579   5.240  2.909     4.8570   5.158\n",
       "183  11.65      13.07       0.8575   5.108  2.850     5.2090   5.135\n",
       "184  12.89      13.77       0.8541   5.495  3.026     6.1850   5.316\n",
       "185  11.56      13.31       0.8198   5.363  2.683     4.0620   5.182\n",
       "186  11.81      13.45       0.8198   5.413  2.716     4.8980   5.352\n",
       "187  10.91      12.80       0.8372   5.088  2.675     4.1790   4.956\n",
       "188  11.23      12.82       0.8594   5.089  2.821     7.5240   4.957\n",
       "189  10.59      12.41       0.8648   4.899  2.787     4.9750   4.794\n",
       "190  10.93      12.80       0.8390   5.046  2.717     5.3980   5.045\n",
       "191  11.27      12.86       0.8563   5.091  2.804     3.9850   5.001\n",
       "192  11.87      13.02       0.8795   5.132  2.953     3.5970   5.132\n",
       "193  10.82      12.83       0.8256   5.180  2.630     4.8530   5.089\n",
       "194  12.11      13.27       0.8639   5.236  2.975     4.1320   5.012\n",
       "195  12.80      13.47       0.8860   5.160  3.126     4.8730   4.914\n",
       "196  12.79      13.53       0.8786   5.224  3.054     5.4830   4.958\n",
       "197  13.37      13.78       0.8849   5.320  3.128     4.6700   5.091\n",
       "198  12.62      13.67       0.8481   5.410  2.911     3.3060   5.231\n",
       "199  12.76      13.38       0.8964   5.073  3.155     2.8280   4.830\n",
       "200  12.38      13.44       0.8609   5.219  2.989     5.4720   5.045\n",
       "201  12.67      13.32       0.8977   4.984  3.135     2.3000     NaN\n",
       "202  11.18      12.72       0.8680   5.009  2.810     4.0510   4.828\n",
       "203  12.70      13.41       0.8874   5.183  3.091     8.4560   5.000\n",
       "204  12.37      13.47       0.8567   5.204  2.960     3.9190   5.001\n",
       "205  12.19      13.20       0.8783   5.137  2.981     3.6310   4.870\n",
       "206  11.23      12.88       0.8511   5.140  2.795     4.3250   5.003\n",
       "207  13.20      13.66       0.8883   5.236  3.232     8.3150   5.056\n",
       "208  11.84      13.21       0.8521   5.175  2.836     3.5980   5.044\n",
       "209  12.30      13.34       0.8684   5.243  2.974     5.6370   5.063\n",
       "\n",
       "[210 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.iloc[:, 1:8]\n",
    "y.unique()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.astype(\"category\", ordered=True,categories=['kama', 'canadian', 'rosa']).cat.codes\n",
    "y.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some basic nan munging. Fill each row's nans with the mean of the feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      area  perimeter  compactness  length  width  asymmetry    groove\n",
      "0    15.26      14.84       0.8710   5.763  3.312     2.2210  5.220000\n",
      "1    14.88      14.57       0.8811   5.554  3.333     1.0180  4.956000\n",
      "2    14.29      14.09       0.9050   5.291  3.337     2.6990  4.825000\n",
      "3    13.84      13.94       0.8955   5.324  3.379     2.2590  4.805000\n",
      "4    16.14      14.99       0.9034   5.658  3.562     1.3550  5.175000\n",
      "5    14.38      14.21       0.8951   5.386  3.312     2.4620  4.956000\n",
      "6    14.69      14.49       0.8799   5.563  3.259     3.5860  5.219000\n",
      "7    14.11      14.10       0.8911   5.420  3.302     2.7000  5.407529\n",
      "8    16.63      15.46       0.8747   6.053  3.465     2.0400  5.877000\n",
      "9    16.44      15.25       0.8880   5.884  3.505     1.9690  5.533000\n",
      "10   15.26      14.85       0.8696   5.714  3.242     4.5430  5.314000\n",
      "11   14.03      14.16       0.8796   5.438  3.201     1.7170  5.001000\n",
      "12   13.89      14.02       0.8880   5.439  3.199     3.9860  4.738000\n",
      "13   13.78      14.06       0.8759   5.479  3.156     3.1360  4.872000\n",
      "14   13.74      14.05       0.8744   5.482  3.114     2.9320  4.825000\n",
      "15   14.59      14.28       0.8993   5.351  3.333     4.1850  4.781000\n",
      "16   13.99      13.83       0.9183   5.119  3.383     5.2340  4.781000\n",
      "17   15.69      14.75       0.9058   5.527  3.514     1.5990  5.046000\n",
      "18   14.70      14.21       0.9153   5.205  3.466     1.7670  4.649000\n",
      "19   12.72      13.57       0.8686   5.226  3.049     4.1020  4.914000\n",
      "20   14.16      14.40       0.8584   5.658  3.129     3.0720  5.176000\n",
      "21   14.11      14.26       0.8722   5.520  3.168     2.6880  5.219000\n",
      "22   15.88      14.90       0.8988   5.618  3.507     0.7651  5.091000\n",
      "23   12.08      13.23       0.8664   5.099  2.936     1.4150  4.961000\n",
      "24   15.01      14.76       0.8657   5.789  3.245     1.7910  5.001000\n",
      "25   16.19      15.16       0.8849   5.833  3.421     0.9030  5.307000\n",
      "26   13.02      13.76       0.8641   5.395  3.026     3.3730  4.825000\n",
      "27   12.74      13.67       0.8564   5.395  2.956     2.5040  4.869000\n",
      "28   14.11      14.18       0.8820   5.541  3.221     2.7540  5.038000\n",
      "29   13.45      14.02       0.8604   5.516  3.065     3.5310  5.097000\n",
      "..     ...        ...          ...     ...    ...        ...       ...\n",
      "180  11.41      12.95       0.8560   5.090  2.775     4.9570  4.825000\n",
      "181  12.46      13.41       0.8706   5.236  3.017     4.9870  5.147000\n",
      "182  12.19      13.36       0.8579   5.240  2.909     4.8570  5.158000\n",
      "183  11.65      13.07       0.8575   5.108  2.850     5.2090  5.135000\n",
      "184  12.89      13.77       0.8541   5.495  3.026     6.1850  5.316000\n",
      "185  11.56      13.31       0.8198   5.363  2.683     4.0620  5.182000\n",
      "186  11.81      13.45       0.8198   5.413  2.716     4.8980  5.352000\n",
      "187  10.91      12.80       0.8372   5.088  2.675     4.1790  4.956000\n",
      "188  11.23      12.82       0.8594   5.089  2.821     7.5240  4.957000\n",
      "189  10.59      12.41       0.8648   4.899  2.787     4.9750  4.794000\n",
      "190  10.93      12.80       0.8390   5.046  2.717     5.3980  5.045000\n",
      "191  11.27      12.86       0.8563   5.091  2.804     3.9850  5.001000\n",
      "192  11.87      13.02       0.8795   5.132  2.953     3.5970  5.132000\n",
      "193  10.82      12.83       0.8256   5.180  2.630     4.8530  5.089000\n",
      "194  12.11      13.27       0.8639   5.236  2.975     4.1320  5.012000\n",
      "195  12.80      13.47       0.8860   5.160  3.126     4.8730  4.914000\n",
      "196  12.79      13.53       0.8786   5.224  3.054     5.4830  4.958000\n",
      "197  13.37      13.78       0.8849   5.320  3.128     4.6700  5.091000\n",
      "198  12.62      13.67       0.8481   5.410  2.911     3.3060  5.231000\n",
      "199  12.76      13.38       0.8964   5.073  3.155     2.8280  4.830000\n",
      "200  12.38      13.44       0.8609   5.219  2.989     5.4720  5.045000\n",
      "201  12.67      13.32       0.8977   4.984  3.135     2.3000  5.407529\n",
      "202  11.18      12.72       0.8680   5.009  2.810     4.0510  4.828000\n",
      "203  12.70      13.41       0.8874   5.183  3.091     8.4560  5.000000\n",
      "204  12.37      13.47       0.8567   5.204  2.960     3.9190  5.001000\n",
      "205  12.19      13.20       0.8783   5.137  2.981     3.6310  4.870000\n",
      "206  11.23      12.88       0.8511   5.140  2.795     4.3250  5.003000\n",
      "207  13.20      13.66       0.8883   5.236  3.232     8.3150  5.056000\n",
      "208  11.84      13.21       0.8521   5.175  2.836     3.5980  5.044000\n",
      "209  12.30      13.34       0.8684   5.243  2.974     5.6370  5.063000\n",
      "\n",
      "[210 rows x 7 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "dtype: int8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for feat in X:\n",
    "    X[feat].fillna(X[feat].mean(), inplace=True)\n",
    "print(X)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split `X` into training and testing data sets using `train_test_split()`. Use `0.33` test size, and use `random_state=1`. This is important so that your answers are verifiable. In the real world, you wouldn't specify a random_state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .. your code here ..\n",
    "data_train, data_test, label_train, label_test = train_test_split(X, y, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of SKLearn's Normalizer class and then train it using its .fit() method against your _training_ data. The reason you only fit against your training data is because in a real-world situation, you'll only have your training data to train with! In this lab setting, you have both train+test data; but in the wild, you'll only have your training data, and then unlabeled data you want to apply your models to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalizer(copy=True, norm='l2')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "norm = preprocessing.Normalizer()\n",
    "norm.fit(data_train)\n",
    "#scaled = pd.DataFrame(scaled, columns = data_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your trained pre-processor, transform both your training AND testing data. Any testing data has to be transformed with your preprocessor that has ben fit against your training data, so that it exist in the same feature-space as the original data used to train your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .. your code here ..\n",
    "traindata = norm.transform(data_train)\n",
    "testdata = norm.transform(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Just like your preprocessing transformation, create a PCA transformation as well. Fit it against your training data, and then project your training and testing features into PCA space using the PCA model's `.transform()` method. This has to be done because the only way to visualize the decision boundary in 2D would be if your KNN algo ran in 2D as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .. your code here ..\n",
    "pca = PCA(n_components = 2, svd_solver='full')\n",
    "pca.fit(traindata)\n",
    "traindata = pca.transform(traindata)\n",
    "testdata = pca.transform(testdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and train a KNeighborsClassifier. Start with `K=9` neighbors. Be sure train your classifier against the pre-processed, PCA- transformed training data above! You do not, of course, need to transform your labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0.871428571429\n",
      "8 0.885714285714\n",
      "7 0.857142857143\n",
      "6 0.842857142857\n",
      "5 0.857142857143\n",
      "4 0.857142857143\n",
      "3 0.871428571429\n",
      "2 0.857142857143\n",
      "1 0.871428571429\n"
     ]
    }
   ],
   "source": [
    "# .. your code here ..\n",
    "for i in range(9, 0, -1):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(traindata, label_train) \n",
    "    print(str(i) + \" \" + str(knn.score(testdata, label_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I hope your KNeighbors classifier model from earlier was named 'knn'\n",
    "# If not, adjust the following line:\n",
    "plotDecisionBoundary(knn, traindata, label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the accuracy score of your test data/labels, computed by your KNeighbors model. You do NOT have to run `.predict` before calling `.score`, since `.score` will take care of running your predictions for you automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87142857142857144"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .. your code here ..\n",
    "knn.score(testdata, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the ordinal conversion, try and get this assignment working with a proper Pandas get_dummies for feature encoding. You might have to update some of the `plotDecisionBoundary()` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEJCAYAAACQZoDoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fd2e0d7cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "58px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
